{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential feature selection algorithm - Class of Greedy serach algorithm\n",
    "\n",
    "An alternative way to reduce the complexity of the model and avoid overfitting is dimensionality reduction via feature selection, which is especially useful for unregularized models.\n",
    "\n",
    "Two main categories of dimensionality  reduction techniques,\n",
    "- feature selection : we select a subset of original feature\n",
    "- feature extraction : we derive infromation from feature set to construct a new feature subspace.\n",
    "\n",
    "In sequential feaature slection algorithm, we reduce initial d-dimensional feature space to a k-dimensional feature subspace where k<d. \n",
    "Reason - automatically selectionn a subset of features which are more relevent to the problem; and imporve computational efficiency or noise, which can be useful for lagorithm that dont support regularization.\n",
    "\n",
    "**Classic sequential feature selection algorithm is - Sequential backward selection**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBS():\n",
    "    def __init__(self, estimator, k_features, scoring = accuracy_score, \n",
    "                 test_size = 0.25, random_state = 1):\n",
    "        self.scoring = scoring\n",
    "        self.estimator = clone(estimator)\n",
    "        self.k_features = k_features\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = self.test_size,\n",
    "                                                            random_state = self.random_state)\n",
    "        \n",
    "        dim = X_train.shape[1]\n",
    "        self.indices_  = tuple(range(dim))\n",
    "        self.subsets_  = [self.indices_]\n",
    "        score = self._calc_score(X_train, y_train, X_test, y_test, self.indices_)\n",
    "        \n",
    "        self.scores_ = [score]\n",
    "        \n",
    "        while dim > self.k_features:\n",
    "            scores = []\n",
    "            subsets = []\n",
    "            \n",
    "            for p in combinations(self.indices_, r = dim -1):\n",
    "                score = self._calc_score(X_train, y_train, X_test, y_test, p)\n",
    "                \n",
    "                scores.append(score)\n",
    "                subsets.append(p)\n",
    "                \n",
    "            best = np.argmax(scores)\n",
    "            self.indices_ = subsets[best]\n",
    "            self.subsets_.append(self.indices_)\n",
    "            dim -= 1\n",
    "            \n",
    "            self.scores_.append(scores[best])\n",
    "            \n",
    "        self.k_score_ = self.scores_[-1]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[:, self.indices_]\n",
    "    \n",
    "    def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n",
    "        self.estimator.fit(X_train[:, indices], y_train)\n",
    "        y_pred = self.estimator.predict(X_test[:, indices])\n",
    "        score = self.scoring(y_test, y_pred)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ClassLabel  Alcohol  MalicAcid   Ash  AlcalinityOfAsh  Magnesium  \\\n",
      "0           1    14.23       1.71  2.43             15.6        127   \n",
      "1           1    13.20       1.78  2.14             11.2        100   \n",
      "2           1    13.16       2.36  2.67             18.6        101   \n",
      "3           1    14.37       1.95  2.50             16.8        113   \n",
      "4           1    13.24       2.59  2.87             21.0        118   \n",
      "\n",
      "   TotalPhenols  Flavanoids  NonflavanoidsPhenols  Proanthocyanins  \\\n",
      "0          2.80        3.06                  0.28             2.29   \n",
      "1          2.65        2.76                  0.26             1.28   \n",
      "2          2.80        3.24                  0.30             2.81   \n",
      "3          3.85        3.49                  0.24             2.18   \n",
      "4          2.80        2.69                  0.39             1.82   \n",
      "\n",
      "   ColorIntensity   Hue  OD280/OD315OfDilutedWines  Proline  \n",
      "0            5.64  1.04                       3.92     1065  \n",
      "1            4.38  1.05                       3.40     1050  \n",
      "2            5.68  1.03                       3.17     1185  \n",
      "3            7.80  0.86                       3.45     1480  \n",
      "4            4.32  1.04                       2.93      735  \n",
      "(178, 14)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/''machine-learning-databases/wine/wine.data', header = None)\n",
    "\n",
    "df_wine.columns = ['ClassLabel', 'Alcohol', 'MalicAcid', 'Ash', 'AlcalinityOfAsh', 'Magnesium', 'TotalPhenols', 'Flavanoids', \n",
    "                  'NonflavanoidsPhenols', 'Proanthocyanins', 'ColorIntensity', 'Hue', 'OD280/OD315OfDilutedWines', 'Proline']\n",
    "\n",
    "print(df_wine.head(5))\n",
    "print(df_wine.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13) (178,) (124, 13) (124,) (54, 13) (54,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X, y = df_wine.iloc[:,1:], df_wine.iloc[:,0] # not in array form\n",
    "X, y = df_wine.iloc[:,1:].values, df_wine.iloc[:,0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0, stratify = y)\n",
    "\n",
    "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train_std = ss.fit_transform(X_train)\n",
    "X_test_std = ss.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SBS at 0x17e3fcac760>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "sbs = SBS(knn, k_features = 1)\n",
    "sbs.fit(X_train_std, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
