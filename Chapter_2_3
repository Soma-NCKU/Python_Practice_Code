# -*- coding: utf-8 -*-
"""
Created on Thu Dec  3 12:52:59 2020

Reference Textbook : Python Machine Learning 
                Sebastian & vahid
                
"""
"""
Large - scale machine learning and Stochastic gradient descent 

For a large dataset with million of data points, which is not uncommon in many machine learning applications. 
Running batch gradient descent can be computationally quite costly, as in such scenarios we need to re-evaluate the 
whole training dataset each time we take one step towards the global minimum.

Important Note;
    A popular alternative to batch gradient descent(BGD) algorithm is stochastic gradient descent (SGD), sometimes called as
    iterative or online gradient descent. 

    Although SGD is considered as an approximation of GD/BGD, it typically reaches convergence much faster because of the more frequent 
    weight updates. Since, each gradient is based on single training example, the error surface is noiser than in GD, which can also have
    advantage that SGD can escape shallow local minima more readily if we are working with nonlinear cost functions.

    To obtain satisfying results from SGD, it is important to present it training data in a random orders. Also, we must shuffle the 
    training set for every epoch to prevent cycles.

Very Important Note:
    In SGD, the fixed learning rate eta is oftain replaced with by an adapative learning rate that decreases over time.
    For example, c_1 / ([no_iter] + c_2). Point to remember is - SGD never reaches global minima, rather an area very close to it.
    
-- Application : online learning ( customer data in web applications)
"""

"""
Mini - batch learning - A compromise between BGD and SGD

"""

import numpy as np

class AdalineSGD(object):
    """
    ADAptive LInear NEuron classifier
    
    Parameters
    -----------
    eta : float
        Learning rate ( between 0.0 and 1.0)
    n_iter : int
        Passes over the training dataset.
    shuffle : bool (default: True)
        Shuffles training data every epoch if True to prevent cyclces.
    random_state : int
        Random number generator seed for random weight initialization.
        
    Attributes
    -----------
    w_ : 1d-array
        Weights after fitting.
    cost_ : list
        Sum-of-squares cost function value averaged over all training samples in each epoch.
        
    """
    
    def __init__(self, eta = 0.01, n_iter = 10, shuffle = True, random_state = None):
        self.eta = eta
        self.n_iter = n_iter
        self.w_initialized = False
        self.shuffle = shuffle
        self.random_state = random_state
        
    def fit(self, X, y):
        """
        Fit training data
        
        Parameters
        -----------
        X : {array-like}, shape = [n_samples, n_features] 
            Training vectors, where n_samples is the number of the samples and 
            n_features is the number of features.
        y : array-like, shape = [n_samples]
            Target values.
            
        Returns
        --------
        self : object
        
        """
        
        self._initialize_weights(X.shape[1])
        self.cost_ = []
        for i in range(self.n_iter):
            if self.shuffle:
                X, y = self._shuffle(X, y)
            cost = []
            for xi, target in zip(X, y):
                cost.append(self._update_weights(xi, target))
            avg_cost = sum(cost) / len(y)
            self.cost_.append(avg_cost)
        return self
    
    def partial_fit(self, X, y):
        """
        Fit training data without reinitializing the weights.
        
        """
        if not self.w_initialized:
            self._initialize_weights(X.shape[1])
        if y.ravel().shape[0] > 1:
            for xi, target in zip(X, y):
                self._update_weights(xi, target)
                
        else:
            self._update_weights(X, y)
            
        return self
    
    def _shuffle(self, X, y):
        """
        Shuffle training data 
        
        """
        # _shuffle method works as follows: via the permuation function in np.random, we generate a random sequence of unique numbers 
        # in range 0 to 100. Those numbers can then be used as indices to shuffle our feature matrix and class label vector.
        
        r = self.rgen.permutation(len(y))
        return X[r], y[r]
    
    def _initialize_weights(self, m):
        """
        Initialize weights to small random numbers
        
        """
        self.rgen = np.random.RandomState(self.random_state)
        self.w_ = self.rgen.normal(loc = 0.0, scale = 0.01, size = 1+m)
        self.w_initialized = True
    
    def _update_weights(self, xi, target):
        """ 
        Apply Adaline learniing rule to update the weights
        
        """
        output = self.activation(self.net_input(xi))
        error = (target - output)
        self.w_[1:] += self.eta * xi.dot(error)
        self.w_[0] += self.eta * error
        cost = 0.5 * error**2
        return cost 
    
    def net_input(self, X):
        """
        Calculate net input 
        
        """
        return np.dot(X, self.w_[1:]) + self.w_[0]
    
    def activation(self, X):
        """
        Compute linear activation
        
        """
        return X
    
    def predict(self, X):
        """
        Return class label after unit step 
        
        """
        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)
    
# ============================================================================

# Data

import pandas as pd 

df = pd.read_csv('https://archive.ics.uci.edu/ml/'
                 'machine-learning-databases/iris/iris.data',
                 header = None)
df.tail()

## tail() function is used to get the last n rows. This function returns last n rows from the object based on position.

print(type(df))

df

# ============================================================================

# Data Visualization

import matplotlib.pyplot as plt

# select setosa and versicolor
y = df.iloc[0:100, 4].values     #iloc[row, col].values - in the form of array; without ".values" - it will be list
y = np.where(y == 'Iris-setosa', -1, 1)

## loc in Pandas is label-based, which means that we have yo specify the name of the rows and cols that we need to filter out whereas 
## iloc in Pnadas is integer index- based, we have to specify rows and cols by their integer index.

# extract sepal length and petal length

X = df.iloc[0:100,[0,2]].values

# plot data

plt.scatter(X[:50, 0], X[:50, 1],
            color = 'red', marker = 'o', label = 'setosa')
plt.scatter(X[50:100, 0], X[50:100, 1],
            color = 'blue', marker = 'x', label = 'versicolor')

plt.xlabel(' sepal length [cm] ')
plt.ylabel(' petal length [cm] ')

plt.legend(loc = 'upper left')
plt.show()

# ===========================================================================+

# Standarization can easily be achieved using the built-in NumPy methods "mean" and "std"

X_std = np.copy(X)
X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()
X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()

# ============================================================================

# Plot the convergence for the algorithm

from matplotlib.colors import ListedColormap

def plot_decision_regions(X, y, classifier, resolution = 0.02):
    
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    
## determine the minimum and maximum values for the two features 
## and use those features vectors to create a pair of grid arrays xx1 and xx2

    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
## Since we trained preceptron classifier on two feature dimensions, we 
## need to flatten the grid arrays and create a matrix that has the same 
## number of columns as the Iris training subset so that we can use the 
## predict method to predict the class labels Z of the corresponding grid points.

    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0],
                    y=X[y == cl, 1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=cl,
                    edgecolor='black')
        

# ===========================================================================
       
ada = AdalineSGD(n_iter = 15, eta = 0.01, random_state = 1)
ada.fit(X_std, y)
#ada.partial_fit(X_std, y)

plot_decision_regions(X_std, y, classifier = ada)
plt.title('Adaline - Gradient Descent')
plt.xlabel('sepal length [standardization]')
plt.ylabel('petal length [standardization]')
plt.legend(loc = 'upper left')
plt.tight_layout()
plt.show()

plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker = 'o')
plt.xlabel('Epochs')
plt.ylabel('Sum-squared-error')
plt.show()
