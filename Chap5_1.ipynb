{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressing Data via Dimensionality Reduction\n",
    "\n",
    "An alternative approach to **feature selection** for dimensionality reduction is **feature extraction.**\n",
    "\n",
    "The **three fundamental technique** that can help summarize the infromation content of a dataset by transforming it onto a new feature subspace of lower dimensionality than original one are PCA, LDA, KPCA.\n",
    "\n",
    "**Data compression is an important topic in machine learning**, and it helps us to store and analyze the increasing amounts of data that are produced and collected in the modern age of technology.\n",
    "\n",
    "- **Principal Component Analysis**(PCA) - for unsupervised data compression.\n",
    "- **Linear Discrimnant Analysis**(LDA) - as a supervised dimensionality reduction technique for maxmizing class separability.\n",
    "- **Kernel Principal Component Analysis**(KPCA) - Nonlinear dimensionality reduction via KPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between feature selection and feature extraction \n",
    "\n",
    "- Feature selection is for filtering irrelevant or redundant features from your dataset. The **key difference between** feature selection and extraction is that **feature selection keeps a subset of the original features** while **feature extraction creates brand new, smaller set of features that still captures most of the useful infromation.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main steps behind PCA :\n",
    "\n",
    "- Unsupervised linear transformation techniques and is widely used across different fields, for feature extraction and dimensionality reduction.\n",
    "- Popular applications are: exploratory data analyses and de-nosing of signals in stock market trading, and analysis of genome data and gene expression levels in field of bioinformatics.\n",
    "\n",
    "- **PCA helps identify patterns in data based on the correlation between features**. It aims to find directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than original one.\n",
    "\n",
    "- **Note**: PCA directions are highly sensitive to data scaling, and we need to standardize the features prior to PCA if the features were measured on different scales and we want to assign equal importance to all features.\n",
    "\n",
    "$x = [x_{1}, x_{2}, ... , x_{d}], x \\epsilon {R}^d$\n",
    "\n",
    "$\\downarrow xW, W \\epsilon {R}^{d*k}$\n",
    "\n",
    "$z = [z_{1}, z_{2}, ... , z_{d}], z \\epsilon {R}^k$\n",
    "\n",
    "As a result of transforming, the original d-dimensional data onto this new k-dimensional subspace.\n",
    "\n",
    "#### Extracting the principal components step by step \n",
    "\n",
    "Let's first tackle the first four steps of PCA,\n",
    "\n",
    "1. Standardizing the data.\n",
    "2. Constructing the covariance matrix.\n",
    "3. Obtaining the eigenvalues and eigenvectors of the covariance matrix.\n",
    "4. Sorting the eigenvalues by decreasing order to rank the eigenvectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0      1     2     3     4    5     6     7     8     9      10    11  \\\n",
      "0     1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29   5.64  1.04   \n",
      "1     1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28   4.38  1.05   \n",
      "2     1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81   5.68  1.03   \n",
      "3     1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18   7.80  0.86   \n",
      "4     1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82   4.32  1.04   \n",
      "..   ..    ...   ...   ...   ...  ...   ...   ...   ...   ...    ...   ...   \n",
      "173   3  13.71  5.65  2.45  20.5   95  1.68  0.61  0.52  1.06   7.70  0.64   \n",
      "174   3  13.40  3.91  2.48  23.0  102  1.80  0.75  0.43  1.41   7.30  0.70   \n",
      "175   3  13.27  4.28  2.26  20.0  120  1.59  0.69  0.43  1.35  10.20  0.59   \n",
      "176   3  13.17  2.59  2.37  20.0  120  1.65  0.68  0.53  1.46   9.30  0.60   \n",
      "177   3  14.13  4.10  2.74  24.5   96  2.05  0.76  0.56  1.35   9.20  0.61   \n",
      "\n",
      "       12    13  \n",
      "0    3.92  1065  \n",
      "1    3.40  1050  \n",
      "2    3.17  1185  \n",
      "3    3.45  1480  \n",
      "4    2.93   735  \n",
      "..    ...   ...  \n",
      "173  1.74   740  \n",
      "174  1.56   750  \n",
      "175  1.56   835  \n",
      "176  1.62   840  \n",
      "177  1.60   560  \n",
      "\n",
      "[178 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# download and read the load using pandas\n",
    "import pandas as pd\n",
    "\n",
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header = None)\n",
    "print(df_wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset and standardize the features\n",
    "# since PCA direction is highly sensitive to data scaling and need to be standardize the feature prior.\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df_wine.iloc[:, 1:].values\n",
    "y = df_wine.iloc[:, 0].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = 0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.fit_transform(X_test)\n",
    "\n",
    "#print(X_train[0], X_train_std[0])\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Constructing the covariance matrix \n",
    "\n",
    "The symmetric $d*d$-dimensional covariance matrix, where d is the number of dimensions in the dataset, stores the pairwise covariance between the different features. \n",
    "\n",
    "**For example**, the covariance between two features $x_{j}$ and $x_{k}$ on the population level can be calculated via the following equation:\n",
    "\n",
    "$ \\sigma_{jk} = \\frac{1}{n}\\sum_{i=1}^{n} ({x_{j}}^{(i)} - \\mu_{j}) ({x_{k}}^{(i)} - \\mu_{k}) $\n",
    "\n",
    "Here, $\\mu_{j} and \\mu_{k}$ are the sample means of features j and k, respectively. **Note:** that the sample mean is zero if we standardized the dataset. \n",
    "\n",
    "A positive covariance between two features indicates that the features increase or decrease together, whereas a negative covariance indicates the vary opposite directions.\n",
    "\n",
    "**For example**, the covariance matrix of three features can then be written as follows,\n",
    "\n",
    "$ \\sum = \\begin{bmatrix}\n",
    "{\\mu_{1}}^{2} & \\mu_{12} & \\mu_{13}\\\\\n",
    "\\mu_{21} & {\\mu_{2}}^{2} & \\mu_{23}\\\\\n",
    "\\mu_{31} & \\mu_{32} & {\\mu_{3}}^{2} \n",
    "\\end{bmatrix} $\n",
    "\n",
    "The eigenvectors of the covariance matrix represent the principal componenets (the directions of maxiamum variance), whereas the corresponding eigenvalues will define their magnitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute covariance matrix of the standardized training dataset.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "cov_mat = np.cov(X_train_std.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Obtaining the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "From the linear algebra, we known that an eigenvector **v** satisfies the following condition: $\\sum{v} = \\lambda{v}$. \n",
    "Here, $\\lambda$ is a scalar: the eigenvalue. \n",
    "\n",
    "**Note:** Using np.linalg.eig function, we perform te eigen decomposition. This function operates on both symmetric and non-symmetric square matrices. However, one may find that it returns complex eigen values in certain cases.\n",
    "\n",
    "A related function, np.linalg.eigh has been implemented to decompose Hermetian matrices, numerically stable approach to work with symmetric matrices such as the covariance matrix, and returns real eigen values.\n",
    "\n",
    "For the wine dataset, we obtain 13 eigenvectors (since we have 13 features) and eigen values from 13*13 - dimensional covaraince matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eigen Values \n",
      "[4.84274532 2.41602459 1.54845825 0.96120438 0.84166161 0.6620634\n",
      " 0.51828472 0.34650377 0.3131368  0.10754642 0.21357215 0.15362835\n",
      " 0.1808613 ]\n"
     ]
    }
   ],
   "source": [
    "# Obtain eigen values and eigen vectors \n",
    "\n",
    "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "print('\\nEigen Values \\n%s' % eigen_vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Sorting the eigenvalues by decreasing order to rank the eigenvectors.\n",
    "\n",
    "**Understand the concept of Variance:**\n",
    "\n",
    "- Since we want to reduce the dimensionality of the dataset by compressing it onto a new feature subspace, we only select the subset of the eigenvectors (principal components) that contains most of the information (variance).\n",
    "\n",
    "- The eigenvalues define the magnitude of the eigenvectors, so we have to sort the eigenvalues by decreasing magnitude; we are interested in the top 'k' most informative eigenvectors based on their eigenvalues.\n",
    "\n",
    "\n",
    "Let's plot the variance explained ratios of the eigenvalues, which is given as,\n",
    "\n",
    "$ \\frac{\\lambda_{j}}{\\sum_{i=1}^{d} \\lambda_{j}} $\n",
    "\n",
    "**Note:** Using Numpy cumsum function , one can calculate the cumulative sum of explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEGCAYAAACO8lkDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAApbklEQVR4nO3deXxV9bX38c9i0KAIRaA+DGqwBZEhDAZERAW9IlYLVVEcq3jBEUvptWqvVXB6Hts6aytS1NjWAawD6nVA6oClIDNRQISriBFrERQQnALr+WPvnB5yhuxAdk5O+L5fr/PK2dPvrL0JWee3h/Uzd0dERCRZg1wHICIidY+Sg4iIpFByEBGRFEoOIiKSQslBRERSNMp1ANXVqlUrLywszHUYIiJ5ZcGCBZ+5e+uo6+ddcigsLGT+/Pm5DkNEJK+Y2YfVWV+nlUREJIWSg4iIpFByEBGRFEoOIiKSQslBRERSKDmIiEiK2JKDmT1oZv8ys3cyLDczu9vMVplZqZn1jisWERGpnjh7DiXAkCzLTwA6hq8LgftijEVERKohtofg3H2mmRVmWWUY8CcPBpSYY2bfM7M27v5JXDGJiMTt0bfWMG3xx9XapkvbZoz/cdeYIto5uXxCuh3wUdJ0WTgvJTmY2YUEvQsOOOCAWglOROqvnfkDHtVbH2wA4LAO+8bSfm3JZXKwNPPSDkvn7pOASQDFxcUauk5Edsm0xR+z7JNNdGnTrMbbPqzDvgzr2Y6zDsvvL7K5TA5lwP5J0+2BtTmKRUR2M13aNGPKRYfnOow6K5fJ4VlgjJk9DhwGbNT1BhGBeE/7ALH1GuqT2JKDmT0GDARamVkZMB5oDODuE4EXgB8Bq4CtwMi4YhGR/BLnaR8Ieg3DeraLpe36Is67lc6sYrkDl8X1+SKS33TaJ7fybjwHEck9nfap/1Q+Q0SqreK0T1x02if31HMQkZ2i0z71m5KDSD2k0z6yq3RaSaQe0mkf2VXqOYjUUzrtI7tCPQcREUmh5CAiIil0WkkkB3TBWOo69RxEckAXjKWuU89BJEd0wVjqMvUcREQkhZKDiIik0GklkQzivGisC8ZS16nnIJJBnBeNdcFY6jr1HESy0EVj2V2p5yAiIimUHEREJIWSg4iIpFByEBGRFEoOIiKSQslBRERS6FZWyVuqbCoSH/UcJG+psqlIfNRzkLymh9RE4qGeg4iIpFByEBGRFEoOIiKSQslBRERSKDmIiEgKJQcREUmh5CAiIilifc7BzIYAdwENgcnufkul5c2BvwAHhLHc6u4PxRmT1B49wSySv2LrOZhZQ+D3wAlAF+BMM+tSabXLgGXu3gMYCNxmZnvEFZPULj3BLJK/quw5mFl74B5gALAd+Dsw1t3Lqti0L7DK3d8P23kcGAYsS1rHgX3MzICmwAagvLo7IXWXnmAWyU9Reg4PAc8CbYB2wHPhvKq0Az5Kmi4L5yW7FzgEWAu8TZB0tlduyMwuNLP5ZjZ/3bp1ET5aRER2RZTk0NrdH3L38vBVArSOsJ2lmeeVpo8HFgNtgZ7AvWaWchLZ3Se5e7G7F7duHeWjRURkV0RJDp+Z2Tlm1jB8nQOsj7BdGbB/0nR7gh5CspHAUx5YBXwAdI4SuIiIxCdKcrgAOB34J/AJMDycV5V5QEcz6xBeZD6D4PRUsjXAsQBmth9wMPB+tNBFRCQuVV6Qdvc1wNDqNuzu5WY2BniZ4FbWB919qZldHC6fCNwIlJjZ2wSnoa5y98+q+1kiIlKzMiYHM7vS3X9rZveQeq0Ad/9ZVY27+wvAC5XmTUx6vxYYXK2IRUQkdtl6DsvDn/NrIxAREak7MiYHd38ufLvV3Z9IXmZmp8UalYiI5FSUC9K/ijhPRETqiWzXHE4AfgS0M7O7kxY1Q08xi4jUa9muOawluN4wFFiQNH8zMC7OoKR2qDCeiGSS7ZrDEmCJmT3q7t/VYkxSSyoK48X1B1yF8UTyV5SS3YVm9v8IKqsWVMx094Nii0pqjQrjiUg6UQvv3UdwnWEQ8Cfgz3EGJSIiuRUlOTRx978B5u4fuvsE4Jh4wxIRkVyKclrpazNrAKwMy2F8DHw/3rBERCSXovQcfg7sBfwMOBQ4BzgvxphERCTHsvYcwqE+T3f3XwJfEpTYFhGRei5rz8HdtwGHhsN4iojIbiLKNYdFwDQzewLYUjHT3Z+KLSoREcmpKMlhX4KR35LvUHJAyUFEpJ6KMtiPrjOIiOxmotytJCIiuxklBxERSaHkICIiKapMDma2n5k9YGYvhtNdzOw/4w9NRERyJcrdSiUExfeuCaffA6YAD8QUkySJc8wFjbcgIplEOa3Uyt2nAtsB3L0c2BZrVJJQMeZCHDTegohkEqXnsMXMWhI824CZ9QM2xhqV7EBjLohIbYuSHH4BPAv8wMxmAa2B4bFGJSIiORXlIbiFZnY0cDBgwAoNGyoiUr9FuVvpMqCpuy9193eApmZ2afyhiYhIrkS5ID3a3b+omHD3z4HRsUUkIiI5FyU5NEgu2R2O8bBHfCGJiEiuRbkg/TIw1cwmEtyxdDHwUqxRiYhITkVJDlcBFwGXEFyQng5MjjMoERHJrSh3K20H7gtfIiKyG6gyOZjZEcAE4MBwfQPc3Q+KNzQREcmVKBekHwBuBwYAfYDi8GeVzGyIma0ws1VmdnWGdQaa2WIzW2pmb0QNXERE4hPlmsNGd3+xug2HdzX9HjgOKAPmmdmz7r4saZ3vAX8Ahrj7GjP7fnU/R0REal6U5PCamf2OYMzobypmuvvCKrbrC6xy9/cBzOxxYBiwLGmds4Cn3H1N2Oa/qhG7iIjEJEpyOCz8WZw0z4FjqtiuHfBR0nRZUlsVOgGNzex1YB/gLnf/U4SYRCL57rvvKCsr4+uvv851KCK1oqCggPbt29O4ceNdaifK3UqDdrJtSzPP03z+ocCxQBNgtpnNcff3dmjI7ELgQoADDjhgJ8OR3VFZWRn77LMPhYWFJD3LKVIvuTvr16+nrKyMDh067FJbUXoOmNmJQFegICmIG6rYrAzYP2m6PbA2zTqfufsWgtLgM4EeBAMKJbj7JGASQHFxceUEI5LR119/rcQguw0zo2XLlqxbt26X24pSeG8iMAK4nKA3cBrBba1VmQd0NLMOZrYHcAZB6e9k04AjzayRme1FcNppeTXiF6mSEoPsTmrq9z3Kraz93f2nwOfufj1wODv2CNIKR4wbQ1B+Yzkw1d2XmtnFZnZxuM5yglIcpcBcYHJY+VVEdsLq1avp1q1bles8+uijien58+fzs5/9LO7QIouyD2vXrmX48JoZVub111/npJNOqpG2ktVkjLkQ5bTSV+HPrWbWFlgPRDqZ5e4vAC9Umjex0vTvgN9FaU9Edl1FcjjrrLMAKC4upri4uIqt6pa2bdvy17/+NddhZFReXl7nY6xKlJ7D8+HzCL8DFgKrgcdjjEmkXvnTn/5EUVERPXr04NxzzwXg/PPP3+EPR9OmTYHgW+zRRx/N6aefTqdOnbj66qt55JFH6Nu3L927d+d///d/s26fbPXq1Rx55JH07t2b3r17849//AOAq6++mjfffJOePXtyxx13JL45b9++ncLCQr744otEGz/84Q/59NNPWbduHaeeeip9+vShT58+zJo1K+Xztm3bxi9/+Uv69OlDUVER999/PwC33347F1xwAQBvv/023bp1Y+vWrUyYMIFzzz2XY445ho4dO/LHP/4x8j4k9y5KSko45ZRTGDJkCB07duTKK69MbD99+nQOP/xwevfuzWmnncaXX34JwEsvvUTnzp0ZMGAATz31VNp/t8MOO4ylS5cmpgcOHMiCBQuYO3cu/fv3p1evXvTv358VK1Yk4jjttNP48Y9/zODBg3eIMdN+vP766wwcOJDhw4fTuXNnzj77bNyDy6rz5s2jf//+9OjRg759+7J58+aMxzgOUe5WujF8+6SZPQ8UuLvGkJa8c/1zS1m2dlONttmlbTPG/7hrxuVLly7l5ptvZtasWbRq1YoNGzZU2eaSJUtYvnw5++67LwcddBCjRo1i7ty53HXXXdxzzz3ceeedkWL7/ve/zyuvvEJBQQErV67kzDPPZP78+dxyyy3ceuutPP/880DwBwqgQYMGDBs2jKeffpqRI0fy1ltvUVhYyH777cdZZ53FuHHjGDBgAGvWrOH4449n+fIdLw8+8MADNG/enHnz5vHNN99wxBFHMHjwYH7+858zcOBAnn76aW6++Wbuv/9+9tprLwBKS0uZM2cOW7ZsoVevXpx44omR9qGyxYsXs2jRIvbcc08OPvhgLr/8cpo0acJNN93EjBkz2HvvvfnNb37D7bffzpVXXsno0aN59dVX+eEPf8iIESPSHr8zzjiDqVOncv311/PJJ5+wdu1aDj30UDZt2sTMmTNp1KgRM2bM4L//+7958sknAZg9ezalpaXsu+++rF69OtJ+LFq0iKVLl9K2bVuOOOIIZs2aRd++fRkxYgRTpkyhT58+bNq0iSZNmmQ8xrt6Z1I6GZODmR3j7q+a2SlpluHu6dOtiCS8+uqrDB8+nFatWgGw7777VrlNnz59aNOmDQA/+MEPGDx4MADdu3fntddei/zZ3333HWPGjGHx4sU0bNiQ9957r8ptRowYwQ033MDIkSN5/PHHE384Z8yYwbJl/35+ddOmTWzevJl99tknMW/69OmUlpYmejQbN25k5cqVdOjQgZKSEoqKirjooos44ogjEtsMGzaMJk2a0KRJEwYNGsTcuXPp2bNntffh2GOPpXnz5gB06dKFDz/8kC+++IJly5YlPu/bb7/l8MMP591336VDhw507NgRgHPOOYdJkyaltHn66adz3HHHcf311zN16lROO+20xH6dd955rFy5EjPju+/+PWrycccdl/bfONt+9O3bl/bt2wPQs2dPVq9eTfPmzWnTpg19+gSVipo1a1blMa5p2XoORwOvAj9Os8wJnpje7T361hqmLf44tvaXfbKJLm2axdb+7iTbN/y4uHvau0caNWrE9u3bE+t8++23iWV77rln4n2DBg0S0w0aNKC8vLzK7Svccccd7LfffixZsoTt27dTUFCQsk5lhx9+OKtWrWLdunU888wz/PrXvwZg+/btzJ49myZNmmTd13vuuYfjjz8+ZdnKlStp2rQpa9fueDd75WNTeTrqPiQfs4YNG1JeXo67c9xxx/HYY4/tsO7ixYsj3dHTrl07WrZsSWlpKVOmTEmcwrn22msZNGgQTz/9NKtXr2bgwIGJbfbee++0bWXbj0yxp4sx2zGuaRmvObj7eDNrALzo7iMrvS6IPbI8MW3xxyz7pGZPVSTr0qYZw3q2i619idexxx7L1KlTWb9+PUDitFJhYSELFiwAYNq0aTt8+4wiyvYbN26kTZs2NGjQgD//+c9s27YNgH322YfNmzenbdfMOPnkk/nFL37BIYccQsuWLQEYPHgw9957b2K9xYsXp2x7/PHHc9999yViee+999iyZQsbN25k7NixzJw5k/Xr1+9wrWTatGl8/fXXrF+/ntdffz3xTbmqfYiiX79+zJo1i1WrVgGwdetW3nvvPTp37swHH3yQuH5TOXkkO+OMM/jtb3/Lxo0b6d69eyKmdu2C/5MlJSWRYqnufnTu3Jm1a9cyb948ADZv3kx5eXnGYxyHrNcc3H27mY0Bpsby6fVElzbNmHLR4bkOQ+qgrl27cs0113D00UfTsGFDevXqRUlJCaNHj2bYsGH07duXY489NuM3zkyibH/ppZdy6qmn8sQTTzBo0KDEOkVFRTRq1IgePXpw/vnn06tXrx22GzFiBH369NnhD9/dd9/NZZddRlFREeXl5Rx11FFMnLjDjYeMGjWK1atX07t3b9yd1q1b88wzzzBu3DguvfRSOnXqxAMPPMCgQYM46qijgOCUyoknnsiaNWu49tpradu27Q7n6jPtQxStW7empKSEM888k2++CcrC3XTTTXTq1IlJkyZx4okn0qpVKwYMGMA776S/g3748OGMHTuWa6+9NjHvyiuv5LzzzuP222/nmGOqqiK0c/uxxx57MGXKFC6//HK++uormjRpwowZMzIe4zhYxZXxjCuYXUtwO+sUIJGi3L3qK2sxKC4u9nQXpHJlxP2zAZQc6qjly5dzyCGH5DoMSWPChAk0bdqUK664Iteh1Dvpfu/NbIG7R75nOcpzDhWnkC5LmueABvsREamnotzKWvOXwUVktzdhwoRchyBZRC281w3owo6F91RaW0SknooyhvR4YCBBcngBOAH4O6DkICJST0UpnzGcYLyFf7r7SIKS2ntm30RERPJZlOTwlbtvB8rNrBnwL3QxWkSkXouSHOaHhff+CCwgKL43N86gROqT/v37V2v95BLSzz77LLfcckvW9a+77jpmzJiRtZ2dUVhYyGeffbbT21dl4MCBaeskJRs1atQOZTt2RVz7U5Mx1iVR7la6NHw70cxeApq5e2m8YYnE445Xqq4vVB3jjutU5ToVFTh3xtChQxk6dGjWdW64oapBGfPX5MmTcx1CVtu2bavzMe6sKCPBTTOzs8xsb3dfrcQgUj3J5bgzlWfOVEK6pKSEMWPGsHHjRgoLCxP1lLZu3cr+++/Pd999t0P57kztTJgwgVtvvTUx3a1bt8STyD/5yU849NBD6dq1a9oCdJWlK4P94Ycf0rFjRz777DO2b9/OkUceyfTp01m9ejWdO3fmvPPOo6ioiOHDh7N169aUNi+55BKKi4vp2rUr48ePT8xP7l00bdqUa665hh49etCvXz8+/fRTgIzlxNevX8/gwYPp1asXF110Eeke+L3vvvt2KPFdUlLC5ZdfnvW4NG3alOuuu47DDjuM2bNn7xBjpv0oLCxk/Pjx9O7dm+7du/Puu+8C8OWXXzJy5Ei6d+9OUVFRorprplLjtSnKaaXbgQHAMjN7wsyGm1nVFbxEJMWiRYu48847WbZsGe+//z6zZs3i66+/ZvTo0Tz33HO8+eab/POf/0zZrnnz5vTo0YM33ngDgOeee47jjz+exo0bJ9aJ0k46Dz74IAsWLGD+/PncfffdiTpQ6Xz22WeJMtgLFy6kuLiY22+/nQMPPJCrrrqKiy++mNtuu40uXbokqsmuWLGCCy+8kNLSUpo1a8Yf/vCHlHZvvvlm5s+fT2lpKW+88QalpanfQbds2UK/fv1YsmQJRx11VGL8h7FjxzJu3DjmzZvHk08+yahRowC4/vrrGTBgAIsWLWLo0KGsWbMmpc3hw4fvkESnTJmSqESb6bhs2bKFbt268dZbbzFgwIDI+9GqVSsWLlzIJZdckkjUN954I82bN+ftt9+mtLSUY445JuMxrm1VJgd3fyM8tXQQMAk4neCitIhUU0V55gYNGiTKMyeXkDYzzjnnnLTbVtT3B3Yop10hajuV3X333Ylv4x999BErV67MuO6cOXMSZbB79uzJww8/zIcffggE5943b97MxIkTd+il7L///omy2eeccw5///vfU9qdOnUqvXv3plevXixdujTtOfw99tgjcQ3l0EMPTfR8ZsyYwZgxY+jZsydDhw5NlBOfOXNm4hiceOKJtGjRIqXN1q1bc9BBBzFnzhzWr1/PihUrErFmOi4NGzbk1FNPTXt8su3HKaeckjb2yy77d/GJFi1aZD3GtSnqQ3BNCEp3jwB6Aw/HGZRIfZWuPDNEGxR+6NCh/OpXv2LDhg0sWLAgbdG3TO0kl/iGoJcBwamuGTNmMHv2bPbaay8GDhyYWJZOpjLYEJzqKisrA4LTJRVjPVRVlvuDDz7g1ltvZd68ebRo0YLzzz8/bQyNGzdObJt87LKVE49yXEeMGMHUqVPp3LkzJ598MmaW9bgUFBTQsGHDlHaq2o+Kf/vk2NOV5s52jGtTlGsOU4DlwDHA74EfuPvlcQcmsruIWkK6adOm9O3bl7Fjx3LSSSel/IHK1k5hYSELFy4EYOHChXzwwQdAUEq6RYsW7LXXXrz77rvMmTMna6yZymADXHXVVZx99tnccMMNjB49OrHNmjVrmD17diKmyqdiNm3axN57703z5s359NNPefHFF7PGUFmmcuJHHXUUjzzyCAAvvvgin3/+edrtTznlFJ555hkee+yxRG+susdlZ/ejcuyff/551mNcm6Jcc3iIICFc7O6vhs88iEgNKSgoSJSQHjBgAAceeGDGdUeMGMFf/vKXtENbZmvn1FNPZcOGDfTs2ZP77ruPTp2Cu6yGDBlCeXk5RUVFXHvttfTr1y9rrMllsIuKiujXrx/vvvsub7zxBvPmzUskiD322IOHHnoIgEMOOYSHH36YoqIiNmzYwCWXXLJDmz169KBXr1507dqVCy64YIeR4qK4++67mT9/PkVFRXTp0iVRSnz8+PHMnDmT3r17M336dA444IC027do0SIxelzfvn136rjs7H78+te/5vPPP6dbt2706NGD1157LeMxrm1Vluyua1SyW6pDJbtza/Xq1Zx00kkZx0uQeNREye4oPQcREdnNKDmISGwKCwvVa8hTGe9WMrPe2TZ094U1H46IiNQF2W5lvS38WQAUA0sAA4qAtwgejBOp89LdLihSX9XUdeSMp5XcfZC7DwI+BHq7e7G7Hwr0AlbVyKeLxKygoID169fX2H8YkbrM3Vm/fj0FBbtexCLKQ3Cd3f3tpA9/x8x67vIni9SC9u3bU1ZWxrp163IdikitKCgooH379rvcTpTksNzMJgN/ARw4h+ChOJE6r3HjxnTooGHQRaorSnIYCVwCjA2nZwL3xRaRiIjkXJTxHL42s4nAC+6+ohZiEhGRHItSW2kosBh4KZzuaWbPxhyXiIjkUJSH4MYDfYEvANx9MVAYW0QiIpJzUZJDubtv3JnGzWyIma0ws1VmdnWW9fqY2TYzG74znyMiIjUrSnJ4x8zOAhqaWUczuweoclBcM2tIUOL7BKALcKaZdcmw3m+Al6sVuYiIxCZKcrgc6Ap8AzwGbAJ+HmG7vsAqd3/f3b8FHgeGZWj/STS6nIhInRHlbqWtwDXhqzraAR8lTZcBhyWvYGbtgJMJBhLqk6khM7sQuBDIWJNdRERqTpXJwcw6AVcQXIROrO/uqWMUVto0zbzKNQzuBK5y923Zat+4+ySC8aspLi5WHQQRkZhFeQjuCWAiMBnYVo22y4D9k6bbA2srrVMMPB4mhlbAj8ys3N2fqcbniIhIDYuSHMrdfWeeiJ4HdDSzDsDHwBnAWckruHuiroGZlQDPKzGIiORelOTwnJldCjxNcFEaAHffkG0jdy83szEEdyE1BB5096VmdnG4fOLOhy0iInGKkhzOC3/+MmmeAwdVtaG7vwC8UGle2qTg7udHiCV21z+3lGVrN0Vef9knm+jSplmMEYmI1L4odyvVm5KWd7zyXpXrLFrzBes2f1Pleu1bNAGgS5tmDOvZbpdjExGpS7INE3qMu79qZqekW+7uT8UXVu4c3al1pPXGHdcp5khERHInW8/haOBV4MdpljlQL5ODiIhkSQ7uPj78ObL2whERkbogygVpzOxEghIaiYFJ3f2GuIISEZHcijKew0RgBEENJANOAw6MOS4REcmhKIX3+rv7T4HP3f164HB2fPJZRETqmSjJ4avw51Yzawt8B9Sb21tFRCRVlGsOz5vZ94DfAQsJ7lSaHGdQIiKSW1EegrsxfPukmT0PFOzsyHAiIpIfsj0El/bht3BZvX0ITkREsvcc0j38VkEPwYmI1GPZHoLTw28iIrupKM85tDSzu81soZktMLO7zKxlbQQnIiK5EeVW1seBdcCpwPDw/ZQ4gxIRkdyKcivrvkl3LAHcZGY/iSkeERGpA6L0HF4zszPMrEH4Oh34n7gDExGR3ImSHC4CHiUYIvQbgtNMvzCzzWYWfcg0ERHJG1EegtunNgIREZG6I8rdSv9ZabqhmY2PLyQREcm1KKeVjjWzF8ysjZl1B+YA6k2IiNRjUU4rnWVmI4C3ga3Ame4+K/bIREQkZ6KcVuoIjAWeBFYD55rZXjHHJSIiORTltNJzwLXufhFwNLASmBdrVCIiklNRHoLr6+6bANzdgdvM7Nl4wxIRkVzK2HMwsysB3H2TmZ1WabGK8omI1GPZTiudkfT+V5WWDYkhFhERqSOyJQfL8D7dtIiI1CPZkoNneJ9uWkRE6pFsF6R7hLWTDGiSVEfJgILYIxMRkZzJNhJcw9oMRERE6o4ozzmIiMhuJtbkYGZDzGyFma0ys6vTLD/bzErD1z/MrEec8YiISDSxJQczawj8HjgB6AKcaWZdKq32AXC0uxcBNwKT4opHRESii7Pn0BdY5e7vu/u3BIMEDUtewd3/4e6fh5NzgPYxxiMiIhHFmRzaAR8lTZeF8zL5T+DFdAvM7EIzm29m89etW1eDIYqISDpxJod0D8qlfT7CzAYRJIer0i1390nuXuzuxa1bt67BEEVEJJ0ohfd2Vhmwf9J0e2Bt5ZXMrAiYDJzg7utjjEdERCKKs+cwD+hoZh3MbA+CWk07VHM1swOAp4Bz3f29GGMREZFqiK3n4O7lZjYGeBloCDzo7kvN7OJw+UTgOqAl8AczAyh39+K4YorbHa/UXH4bd1ynGmtLRKS64jythLu/ALxQad7EpPejgFFxxiAiItWnJ6RFRCSFkoOIiKRQchARkRRKDiIikkLJQUREUig5iIhICiUHERFJoeQgIiIplBxERCSFkoOIiKRQchARkRSx1laSmqXCfiJSW9RzEBGRFEoOIiKSQslBRERSKDmIiEgKJQcREUmh5CAiIimUHEREJIWSg4iIpFByEBGRFHpCWhL0BLaIVFDPQUREUqjnILVGPROR/KGeg4iIpFDPQeoF9UpEapZ6DiIikkLJQUREUui0kkgEOm0luxv1HEREJIV6DiJ1gHomUtcoOYjsBpR8pLqUHERklyjx1E+xJgczGwLcBTQEJrv7LZWWW7j8R8BW4Hx3XxhnTCKSX+JOPkpu6cWWHMysIfB74DigDJhnZs+6+7Kk1U4AOoavw4D7wp8iIvVCviafOO9W6guscvf33f1b4HFgWKV1hgF/8sAc4Htm1ibGmEREJAJz93gaNhsODHH3UeH0ucBh7j4maZ3ngVvc/e/h9N+Aq9x9fqW2LgQuDCcPBtYDn8USeO1oRf7Gn8+xQ37Hn8+xQ37Hn8+xQxD/3u7eOuoGcV5zsDTzKmeiKOvg7pOASYmNzOa7e/GuhZc7+Rx/PscO+R1/PscO+R1/PscOifgLq7NNnKeVyoD9k6bbA2t3Yh0REallcSaHeUBHM+tgZnsAZwDPVlrnWeCnFugHbHT3T2KMSUREIojttJK7l5vZGOBlgltZH3T3pWZ2cbh8IvACwW2sqwhuZR0ZsflJVa9Sp+Vz/PkcO+R3/PkcO+R3/PkcO+xE/LFdkBYRkfylwnsiIpJCyUFERFLkXXIwsyFmtsLMVpnZ1bmOJyoz29/MXjOz5Wa21MzG5jqmnWFmDc1sUfiMSt4ws++Z2V/N7N3w3+DwXMdUHWY2Lvy9ecfMHjOzglzHlI2ZPWhm/zKzd5Lm7Wtmr5jZyvBni1zGmEmG2H8X/u6UmtnTZva9HIaYVbr4k5ZdYWZuZq2qaievkkNSSY4TgC7AmWbWJbdRRVYO/Je7HwL0Ay7Lo9iTjQWW5zqInXAX8JK7dwZ6kEf7YGbtgJ8Bxe7ejeAGjzNyG1WVSoAhleZdDfzN3TsCfwun66ISUmN/Bejm7kXAe8CvajuoaighNX7MbH+CckZrojSSV8mBaCU56iR3/6SiqKC7byb449Qut1FVj5m1B04EJuc6luows2bAUcADAO7+rbt/kdOgqq8R0MTMGgF7UcefB3L3mcCGSrOHAQ+H7x8GflKbMUWVLnZ3n+7u5eHkHIJnsuqkDMce4A7gStI8aJxOviWHdsBHSdNl5NkfWAAzKwR6AW/lOJTqupPgl2t7juOoroOAdcBD4SmxyWa2d66DisrdPwZuJfjG9wnB80DTcxvVTtmv4jmm8Of3cxzPzroAeDHXQVSHmQ0FPnb3JVG3ybfkEKncRl1mZk2BJ4Gfu/umXMcTlZmdBPzL3RfkOpad0AjoDdzn7r2ALdTdUxopwnPzw4AOQFtgbzM7J7dR7Z7M7BqCU8SP5DqWqMxsL+Aa4LrqbJdvySGvy22YWWOCxPCIuz+V63iq6QhgqJmtJjidd4yZ/SW3IUVWBpS5e0VP7a8EySJf/Afwgbuvc/fvgKeA/jmOaWd8WlF1Ofz5rxzHUy1mdh5wEnC259cDYj8g+GKxJPz/2x5YaGb/J9tG+ZYcopTkqJPCgY0eAJa7++25jqe63P1X7t4+LN51BvCqu+fFt1d3/yfwkZkdHM46FliWZZO6Zg3Qz8z2Cn+PjiWPLqgneRY4L3x/HjAth7FUSzhw2VXAUHffmut4qsPd33b377t7Yfj/twzoHf6/yCivkkN4QaiiJMdyYKq7L81tVJEdAZxL8I17cfj6Ua6D2o1cDjxiZqVAT+D/5jac6MIez1+BhcDbBP9v63Q5BzN7DJgNHGxmZWb2n8AtwHFmtpLgrplbsrWRKxlivxfYB3gl/L87MadBZpEh/uq3k1+9IxERqQ151XMQEZHaoeQgIiIplBxERCSFkoOIiKRQchARkRRKDlIjzGxbeIvfO2b2RPhUZrr1/hGhrck7W5TQzCaY2RU7s219YGaFZnZWhmVtzeyv1WzvfDO7t2aik3yi5CA15St37xlWDf0WuDh5YVhRF3ev8sledx/l7vn0kFpdUgikTQ7uvtbdh9duOJKvlBwkDm8CPzSzgeEYFo8SPLyFmX0Z/hxoZq8njbHwSPj0L+H84or1zew2M1toZn8zs9bh/NFmNs/MlpjZk5l6KhXMbL+wDv+S8NU/nP+LsLfzjpn9PJxXGMY0OZz/iJn9h5nNsmAsgr7hehPM7M9m9mo4f3Q43yyo//+Omb1tZiMi7POhZvaGmS0ws5eTyky8bma/MbO5ZvaemR0Zzm8YfsY8C8YYuCjc1VuAI8Ne3LhKx6DQwhr/YY/gKTN7KYz9t0nrjQw/6w2Chzcr5rcOj/W88HVEOH+amf00fH+RmeVN3SHJwt310muXX8CX4c9GBGURLgEGEhS565BmvYHARoI6Lw0InugcEC57nWDsAggKK54dvr8OuDd83zKpzZuAy8P3E4Ar0sQ3haDYIQTjITQHDiVIWnsDTYGlBNVyCwmKq3UPY1sAPEhQ+HEY8EzSZy0BmgCtCCoGtwVOJaj/3xDYj6D8RZtM+ww0Bv4BtA7bHQE8mHQsbgvf/wiYEb6/EPh1+H5PYD5B/ZyBwPMZ/o0KgXfC9+cD74fHoQD4kKBuWZsw3tbAHsCspGP+aNK/0QEEpWAI93EVcCTBWAf75vr3Ua9dfzVCpGY0MbPF4fs3CepI9QfmuvsHGbaZ6+5lAOG2hcDfK62zneAPO8BfCIrOAXQzs5uA7xH8YX+5iviOAX4K4O7bgI1mNgB42t23hDE8RfAH7lmCQncVvZ2lBIPUuJm9HcZZYZq7fwV8ZWavEYw5MgB4LPycT8Nv4H2ATRn2+QugG0FpBgiSyidJn1GxzwuSPnswUGRmFaeJmgMdCU7pRfU3d98YxrIMOJAgyb3u7uvC+VOATuH6/wF0CWMEaGZm+7j7p2Z2HfAacLK7pxtLQPKMkoPUlK/cvWfyjPCPyJYs23yT9H4b0X4fK+q9lAA/cfclZnY+wTfm6kpXAj5dbNuTprezY5yV6894Ndqt2GcDlrp7pqFLv6m0PuE2l7v7DknRzAZm+ewosUDmMvgNgMPDZFhZd2A9Qc9J6gFdc5C6rgFQ8e34LP7ds9gH+MSCMuhnR2jnbwSnuirO1zcDZgI/saDa6d7AyQS9nuoYZmYFZtaSIEHNC9sdEX5Oa4JR6OZmaWMF0NrCca3NrLGZda3ic18GLgn3HzPrFO7DZoJjs7PeAgaaWcuw7dOSlk0nKHxJ+Jk9w599CYbu7QVcYWYdduHzpY5QcpC6bgvQ1cwWEJwauiGcfy3BH7JXgHcjtDMWGBSeFloAdPVg2NYSgj/cbwGT3X1RNeObC/wPwdCRN7r7WuBpoJTgesSrwJWepTyyB0PeDgd+Y2ZLgMVUPV7DZIKy4wvDi8z3E3zzLwXKw4vu47I1kCGWTwiupcwGZhBUgq3wM6A4vAC+DLjYzPYE/ghcEO77fwEPVlxol/ylqqxSp5nZl+7eNNdxpGNmEwgusN+a61hEapp6DiIikkI9BxERSaGeg4iIpFByEBGRFEoOIiKSQslBRERSKDmIiEiK/w/Y71VWMzjgRgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sorting the eigen values\n",
    "\n",
    "denominator = sum(eigen_vals)\n",
    "\n",
    "var_exp = [(i/denominator) for i in sorted(eigen_vals, reverse = True)]\n",
    "\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(range(1,14), var_exp, alpha = 0.5, align = 'center', label = 'individual explained variance')\n",
    "plt.step(range(1,14), cum_var_exp, where = 'mid', label = 'cumulative explained variance')\n",
    "\n",
    "plt.xlabel('Prinipal componenet index')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation \n",
    "\n",
    "Let's proceed with the last three steps to transform the dataset onto the new principal component axes. \n",
    "\n",
    "5. Select 'k' eigenvectors, which correspond to the k largest eigenvalues, where k $\\le$ d.\n",
    "6. Construct a projection matrix W from the 'top' k eigenvectors.\n",
    "7. Transform the d-dimensional input dataset X using the projection matrix W to obtain the new k-dimensional feature subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Select 'k' eigenvectors, which correspond to the k largest eigenvalues, where k $\\le$ d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i]) for i in range(len(eigen_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low \n",
    "\n",
    "eigen_pairs.sort(key = lambda k: k[0], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
