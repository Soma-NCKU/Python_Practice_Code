# Visualization
%matplotlib inline
import matplotlib.pyplot as plt
import missingno
import seaborn as  sns                    # graph library
plt.style.use('seaborn-whitegrid')

# Data Manipulation
import numpy as np
import pandas as pd   

# Preprocessing
#from sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize     # for the wine df this is not needed
from sklearn.preprocessing import StandardScaler

# Machine learning
from sklearn.model_selection import train_test_split
from sklearn.base import clone
from itertools import combinations
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier


# Data manipulator
import numpy as np
import pandas as pd

# =====================================================================================================================


# Sequential Feature Selection (Sequential Backward Selection) 

class SBS():
    def __init__(self, estimator, k_features, scoring = accuracy_score, test_size = 0.25, random_state = 1):
        self.estimator = clone(estimator)
        self.k_features = k_features
        self.scoring = scoring
        self.test_size = test_size
        self.random_state = random_state
        
    def fit(self, X, y):
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = self.test_size, random_state = self.random_state)
        dim = X_train.shape[1]
        self.indices_ = tuple(range(dim))
        self.subsets_ = [self.indices_]
        score = self._calc_score(X_train, y_train, X_val, y_val, self.indices_)
        self.scores_ = [score]
        
        while dim > self.k_features:
            scores = []
            subsets = []
            for p in combinations(self.indices_, r = dim -1):
                score = self._calc_score(X_train, y_train, X_val, y_val, p)
                scores.append(score)
                subsets.append(p)
                
            best = np.argmax(scores)
            self.indices_ = subsets[best]
            self.subsets_.append(self.indices_)
            dim = dim - 1
            self.scores_.append(scores[best])
        self.k_scores_ = self.scores_[-1]
        
        return self
    
    def transform(self, X):
        return X[:, self.indices_]
    
    def _calc_score(self, X_train, y_train, X_test, y_test, indices):
        self.estimator.fit(X_train[:, indices], y_train)
        y_pred = self.estimator.predict(X_test[:, indices])
        score = self.scoring(y_test, y_pred)
        return score        
       
       
# ===================================================================================================

df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/''machine-learning-databases/wine/wine.data', header = None)

df_wine.columns = ['ClassLabel', 'Alcohol', 'MalicAcid', 'Ash', 'AlcalinityOfAsh', 'Magnesium', 'TotalPhenols', 'Flavanoids', 
                  'NonflavanoidsPhenols', 'Proanthocyanins', 'ColorIntensity', 'Hue', 'OD280/OD315OfDilutedWines', 'Proline']

print(df_wine.head(5))
print(df_wine.shape)

print('ClassLabel', np.unique(df_wine['ClassLabel']))


# Dataset contains only Ordinal feature variables 
X = df_wine.iloc[:, 1:].values
y = df_wine.iloc[:, 0].values

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0, stratify = y)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

df_wine.describe()  # quick function for dataframe X_train or X.describe wont work

missingno.matrix(df_wine, figsize = (36, 5))   # shows no missing data

# ========================================================================================================

# Standarize the dataset
sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

# Train the Classifier 
knn = KNeighborsClassifier(n_neighbors = 5)
sbs = SBS(knn, k_features = 1)           # k_features means desired number of feature we want to return, atleast one
sbs.fit(X_train_std, y_train)

# Plot the classification acccuracy 

k_feat = [len(k) for k in sbs.subsets_]

plt.plot(k_feat, sbs.scores_, marker = 'o')
plt.ylim([0.7, 1.02])
plt.ylabel('Accuracy')
plt.xlabel('Number of features')
plt.grid()
plt.show()

# smallest feature subset (k = 3, 7 - 12) - 100 % accuracy

k3 = list(sbs.subsets_[10])
print(df_wine.columns[1:][k3])

k3

knn.fit(X_train_std, y_train)
print("Training accuracy:", knn.score(X_train_std, y_train))
print("Test accuracy:", knn.score(X_test_std, y_test))

knn.fit(X_train_std[:, k4], y_train)
print("Training accuracy:", knn.score(X_train_std[:, k3], y_train))
print("Test accuracy:", knn.score(X_test_std[:, k3], y_test))

# =======================================================================================================

"""

We observe that the accuracy of the test set declined slightly. 
This may indicates that those three features do not provide less discriminatory infromation than original dataset. 
Also we must keep in mind that Wine dataset is very small hence are susceptible to randomness (i.e. how the dataset is split)

Even though we didnt improve the performance of the KNN model by reducing the number of features, we shrank the size of the dataset, whuich is useful in
real world application that involves expensive data collection steps. Substantially reducing number of features, we obtain simpler models
whihc are easier to interpret.

Feature selection in scikit learn :

-- Recursive backward elimination based on feature weights, tree-based methods to select features by importance and univariate
statistical tests. 

refer:
http://scikit-learn.org/stable/modules/feature_selection.html

http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/.


"""
