# Wine Dataset
import numpy as np
import pandas as pd

df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/''machine-learning-databases/wine/wine.data', header = None)

df_wine.columns = ['ClassLabel', 'Alcohol', 'MalicAcid', 'Ash', 'AlcalinityOfAsh', 'Magnesium', 'TotalPhenols', 'Flavanoids', 
                  'NonflavanoidsPhenols', 'Proanthocyanins', 'ColorIntensity', 'Hue', 'OD280/OD315OfDilutedWines', 'Proline']


print(df_wine.head(5))
print(df_wine.shape)

print('ClassLabel', np.unique(df_wine['ClassLabel']))

# Randomly partition the dataset

from sklearn.model_selection import train_test_split

#X, y = df_wine.iloc[:,1:], df_wine.iloc[:,0] # not in array form
X, y = df_wine.iloc[:,1:].values, df_wine.iloc[:,0].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0, stratify = y)

X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape

# Bringing the features onto the same scale

# feature scaling is a crucial step. 

# Note: Among few ML algorithm, Decision tree and Random forest are scale invarient.

# An example , Squared error funstion on Adaline, intuitively, we say that algorithm is mostly busy 
# optimizing the weights according to the large error in the second feature. Also, in KNN with euclidean
# distance measure; the computed distance between samples will be dominated by the second feature axis.

# two common approaches - Normalization and Standardization

# Although Normalization via min-max scaling is commonly used technique that is useful when we need values in 
# bounded interval; Standardization can be more practical for many ML algorithms, espicalling for optimization
# algorithm such as GD. The reason is that many algo such as logistic regression, SVM, initialize weights to 0 
# or small random values close to zero. 

# Using standardization, we center the feature columns at mean 0 with standard deviation 1 so that the features 
# columns takes the form of a normal distribution, which make its easier to learn weights. Standarization maintains useful
# infromation about the outlier and makes the algorithm less sensitive to them in contrast to min-max scaling which scales
# the data to a limited range.

ex = np.array([0, 1,2, 3, 4, 5])

print('standardized:', (ex - ex.mean()) / ex.std())
print('normalized:', (ex - ex.min()) / (ex.max() - ex.min()))

# Normalization
from sklearn.preprocessing import MinMaxScaler 
mms = MinMaxScaler()
X_train_norm = mms.fit_transform(X_train)
X_test_norm = mms.fit_transform(X_test)

# Standardization
from sklearn.preprocessing import StandardScaler
stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
X_test_std = stdsc.fit_transform(X_test)

print(X_train_norm)
print('-'* 80)
print(X_train_std)

# ------------------------------------------------------------------------------------------------------------------

# Train a model 
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(C = 0.5, random_state = 1)
lr.fit(X_train_std, y_train)

y_pred = lr.predict(X_test_std)
print('Misclassified samples: %d' %(y_test != y_pred).sum())

from sklearn.metrics import accuracy_score
print('Accuracy: %f' % accuracy_score(y_test, y_pred))

# Alternatively, each classifier in scikit-learn has a score method, as shown below
print('Accuracy: %f' % lr.score(X_test_std, y_test))


from sklearn.svm import SVC
svm = SVC(kernel = 'linear', C = 0.5, random_state = 1)
svm.fit(X_train_std, y_train)

y_pred = svm.predict(X_test_std)
print('Misclassified samples: %d' %(y_test != y_pred).sum())

from sklearn.metrics import accuracy_score
print('Accuracy: %f' % accuracy_score(y_test, y_pred))

# Alternatively, each classifier in scikit-learn has a score method, as shown below
print('Accuracy: %f' % svm.score(X_test_std, y_test))

# Train a model 
from sklearn.svm import SVC
svm = SVC(kernel = 'rbf', C = 0.5, gamma = 0.1, random_state = 1)
svm.fit(X_train_std, y_train)
y_pred = svm.predict(X_test_std)
print('Misclassified samples: %d' %(y_test != y_pred).sum())

from sklearn.metrics import accuracy_score
print('Accuracy: %0.2f' % accuracy_score(y_test, y_pred))

# Alternatively, each classifier in scikit-learn has a score method, as shown below
print('Accuracy: %0.2f' % svm.score(X_test_std, y_test))

from sklearn.ensemble import RandomForestClassifier

forest = RandomForestClassifier(criterion = 'gini', n_estimators = 25, random_state = 1, n_jobs = 2)
forest.fit(X_train, y_train)

y_pred = forest.predict(X_test_std)
print('Misclassified samples: %d' %(y_test != y_pred).sum())

from sklearn.metrics import accuracy_score
print('Accuracy: %f' % accuracy_score(y_test, y_pred))

# Alternatively, each classifier in scikit-learn has a score method, as shown below
print('Accuracy: %f' % forest.score(X_test_std, y_test))

from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier(criterion = 'gini', max_depth = 4, random_state = 1)
dtree = tree.fit(X_train, y_train)

y_pred = tree.predict(X_test_std)
print('Misclassified samples: %d' %(y_test != y_pred).sum())

from sklearn.metrics import accuracy_score
print('Accuracy: %f' % accuracy_score(y_test, y_pred))

# Alternatively, each classifier in scikit-learn has a score method, as shown below
print('Accuracy: %f' % tree.score(X_test_std, y_test))

# ------------------------------------------------------------------------------------------------

## Selecting meaningful features -- Using REGULARIZATION

# Use L1 or L2 

# We notice that model performs much better on training dataset than on the test dataset, 
# this observation is a strong indicator of OVERFITTING.

# It indicates the model fits too closely wrt particular observation in training dataset, 
# but does not generalize well to new data, therefore it has high variance. -- (model is said to be
# too complex)

# Common solution: 1. Collect more training data; 2. Introduce penalty (via regularization)
#                  3. Choose simpler model with fewer parameter 4. Reduce dimensionality of data


# L2 and L1 both reduces the complexity of a model. 

# Note: In contrast L2 regularization, L1 regularization usually yields spare feature vectors; most feature
# weights will be zero. Sparity is useful if we have high - dimensional dataset with many features that are
# irrelevant. In sense L1, can be understood as a technique for feature selection.


# Let us consider the model of LOGISTRIC REGRESSION
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(penalty = 'l1', C = 0.6, solver='liblinear')  # choosing lower values of C ( = 1 / alpha)
lr.fit(X_train_std, y_train)

print('Training accuracy: ', lr.score(X_train_std, y_train))

print('Test accuracy: ', lr.score(X_test_std, y_test))

lr.intercept_
lr.coef_

# -----------------------------------------------------------------------------------------------------------
# Visualization

import matplotlib.pyplot as plt

fig = plt.figure()
ax = plt.subplot(111)

colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'pink', 'lightgreen',
         'gray', 'lightblue', 'gray', 'indigo', 'orange']

weights, params = [], []

for c in np.arange(-4, 6):
    lr = LogisticRegression(penalty = 'l1', C = 10.0**c, random_state = 0, solver='liblinear')
    lr.fit(X_train_std, y_train)
    weights.append(lr.coef_[1])
    params.append(10.0**c)
    
weights = np.array(weights)

for column, color in zip(range(weights.shape[1]), colors):
    plt.plot(params, weights[:, column],
            label = df_wine.columns[column + 1],
            color = color)
    
plt.axhline(0, color = 'black', linestyle = "--", linewidth = 3)
plt.xlim([10**(-5), 10**(5)])
plt.ylabel('weight coefficient')
plt.xlabel('C')
plt.xscale('log')
plt.legend(loc = 'upper left')
plt.legend(loc = 'upper center', bbox_to_anchor = (1.38, 1.03),
          ncol = 1, fancybox = True)
plt.show()

# Plot provides an insights into the behavior of L1 regularization. 
# All features weights will be zero if we penalize the model with strong regularization parameters(C<0.1);
# C is the inverse of regularizaation parameter.

